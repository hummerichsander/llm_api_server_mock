# OpenAI API server mockup
This is a simple fastapi based server mock that implements the OpenAI API.

Available endpoints:
- /v1/chat/completion

Instead of running a LLM model to generate completions, it simply returns a response generated by surrogate models. Available surrogate models are:
- "yes_no": returns random "yes" or "no" response
- "lorem_ipsum": returns random "lorem ipsum" text

## Run via docker:
```bash
docker pull ghcr.io/hummerichsander/openai_api_server_mock:v ... # replace ... with the latest version
```
